#!/usr/bin/env python3
"""
æ‰¹é‡ä¸‹è½½2025å¹´å¹´åº¦æŠ¥å‘Šå¹¶å®ç°è§£æ
Batch download and parse 2025 annual reports
"""

import asyncio
import json
import sys
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.core.logging import get_logger
from src.models.database import ReportType
from src.scrapers.fund_scraper import FundReportScraper
from src.services.qdii_fund_service import QDIIFundService
from src.parsers.xbrl_parser import XBRLParser

logger = get_logger(__name__)


class AnnualReportBatchProcessor:
    """
    2025å¹´å¹´åº¦æŠ¥å‘Šæ‰¹é‡å¤„ç†å™¨
    2025 Annual Report Batch Processor
    """

    def __init__(self):
        self.scraper = FundReportScraper()
        self.qdii_service = QDIIFundService()
        self.parser = XBRLParser()

        logger.info("annual_processor.initialized")

    async def get_all_annual_reports(
        self,
        year: int = 2024,
        max_pages: Optional[int] = None,
        fund_type_filter: Optional[str] = None,
    ) -> List[Dict]:
        """
        è·å–æŒ‡å®šå¹´ä»½çš„æ‰€æœ‰å¹´åº¦æŠ¥å‘Š
        Get all annual reports for specified year
        """
        logger.info(
            "annual_processor.get_all_reports.start",
            year=year,
            max_pages=max_pages,
            fund_type_filter=fund_type_filter,
        )

        all_reports = []
        page = 1

        while True:
            if max_pages and page > max_pages:
                logger.info(
                    "annual_processor.max_pages_reached", page=page, max_pages=max_pages
                )
                break

            try:
                # ä½¿ç”¨å¹´åº¦æŠ¥å‘Šçš„å‚æ•°
                reports, has_next = await self.scraper.get_report_list(
                    year=year,
                    report_type=ReportType.ANNUAL,  # å¹´åº¦æŠ¥å‘Š
                    page=page,
                    page_size=100,
                    fund_type=fund_type_filter,  # å¯ä»¥ç­›é€‰ç‰¹å®šåŸºé‡‘ç±»å‹
                )

                all_reports.extend(reports)

                logger.info(
                    "annual_processor.page_complete",
                    page=page,
                    page_reports=len(reports),
                    total_reports=len(all_reports),
                    has_next=has_next,
                )

                if not has_next:
                    logger.info("annual_processor.last_page_reached")
                    break

                page += 1

                # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
                await asyncio.sleep(0.5)

            except Exception as e:
                logger.error("annual_processor.page_error", page=page, error=str(e))
                break

        logger.info(
            "annual_processor.get_all_reports.complete",
            total_reports=len(all_reports),
            total_pages=page - 1,
        )

        return all_reports

    async def batch_download_and_parse(
        self,
        reports: List[Dict],
        save_dir: Optional[Path] = None,
        max_concurrent: int = 3,
        parse_xbrl: bool = True,
    ) -> Dict:
        """
        æ‰¹é‡ä¸‹è½½å’Œè§£ææŠ¥å‘Š
        Batch download and parse reports
        """
        if save_dir is None:
            save_dir = Path("data/annual_reports_2025")

        save_dir.mkdir(parents=True, exist_ok=True)

        logger.info(
            "annual_processor.batch_process.start",
            reports_count=len(reports),
            save_dir=str(save_dir),
            max_concurrent=max_concurrent,
            parse_xbrl=parse_xbrl,
        )

        results = {
            "total_reports": len(reports),
            "successful_downloads": 0,
            "failed_downloads": 0,
            "successful_parsing": 0,
            "failed_parsing": 0,
            "download_details": [],
            "parsing_results": [],
            "errors": [],
            "statistics": {
                "fund_companies": {},
                "fund_types": {},
                "qdii_funds": 0,
                "total_file_size": 0,
            },
        }

        # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°
        semaphore = asyncio.Semaphore(max_concurrent)

        async def process_single_report(report: Dict, index: int) -> Dict:
            """å¤„ç†å•ä¸ªæŠ¥å‘Š"""
            async with semaphore:
                return await self._process_single_report(
                    report, index, len(reports), save_dir, parse_xbrl
                )

        # å¹¶å‘å¤„ç†æ‰€æœ‰æŠ¥å‘Š
        tasks = [process_single_report(report, i) for i, report in enumerate(reports)]

        completed_results = await asyncio.gather(*tasks, return_exceptions=True)

        # æ±‡æ€»ç»“æœ
        for result in completed_results:
            if isinstance(result, Exception):
                results["errors"].append(str(result))
                results["failed_downloads"] += 1
            elif isinstance(result, dict):
                results["download_details"].append(result)

                if result.get("download_success"):
                    results["successful_downloads"] += 1
                    results["statistics"]["total_file_size"] += result.get(
                        "file_size", 0
                    )

                    # ç»Ÿè®¡åŸºé‡‘å…¬å¸
                    company = result.get("fund_company", "Unknown")
                    results["statistics"]["fund_companies"][company] = (
                        results["statistics"]["fund_companies"].get(company, 0) + 1
                    )

                    # æ£€æŸ¥æ˜¯å¦ä¸ºQDIIåŸºé‡‘
                    fund_name = result.get("fund_name", "")
                    if self.qdii_service.is_qdii_fund(fund_name):
                        results["statistics"]["qdii_funds"] += 1
                else:
                    results["failed_downloads"] += 1

                if result.get("parsing_success"):
                    results["successful_parsing"] += 1
                    results["parsing_results"].append(result.get("parsing_result"))
                elif result.get("parsing_attempted"):
                    results["failed_parsing"] += 1

        # ä¿å­˜ç»“æœæ±‡æ€»
        self._save_batch_results(results, save_dir)

        logger.info(
            "annual_processor.batch_process.complete",
            total_reports=results["total_reports"],
            successful_downloads=results["successful_downloads"],
            failed_downloads=results["failed_downloads"],
            successful_parsing=results["successful_parsing"],
            failed_parsing=results["failed_parsing"],
            success_rate=f"{(results['successful_downloads'] / results['total_reports'] * 100):.1f}%",
        )

        return results

    async def _process_single_report(
        self, report: Dict, index: int, total: int, save_dir: Path, parse_xbrl: bool
    ) -> Dict:
        """å¤„ç†å•ä¸ªæŠ¥å‘Š"""
        upload_info_id = report.get("upload_info_id")
        fund_code = report.get("fund_code", "unknown")
        fund_name = report.get("fund_short_name", "unknown")
        fund_company = report.get("organ_name", "unknown")

        result = {
            "fund_code": fund_code,
            "fund_name": fund_name,
            "fund_company": fund_company,
            "upload_info_id": upload_info_id,
            "index": index + 1,
            "total": total,
            "download_success": False,
            "parsing_attempted": False,
            "parsing_success": False,
        }

        try:
            logger.info(
                "annual_processor.processing_report",
                progress=f"{index+1}/{total}",
                fund_code=fund_code,
                fund_name=fund_name,
            )

            if not upload_info_id:
                raise ValueError("Missing upload_info_id")

            # ä¸‹è½½XBRLå†…å®¹
            content = await self.scraper.download_xbrl_content(upload_info_id)

            # ä¿å­˜æ–‡ä»¶
            file_path = self._save_report_file(content, fund_code, "ANNUAL", save_dir)

            result.update(
                {
                    "download_success": True,
                    "file_path": str(file_path),
                    "file_size": len(content),
                    "download_time": datetime.now().isoformat(),
                }
            )

            # è§£æXBRLå†…å®¹
            if parse_xbrl:
                result["parsing_attempted"] = True
                try:
                    parsing_result = await self._parse_xbrl_content(content, fund_code)
                    result.update(
                        {
                            "parsing_success": parsing_result.get(
                                "parsing_success", False
                            ),
                            "parsing_result": parsing_result,
                        }
                    )
                except Exception as e:
                    result["parsing_error"] = str(e)

            logger.info(
                "annual_processor.report_success",
                fund_code=fund_code,
                file_size=len(content),
                parsing_success=result.get("parsing_success", False),
            )

        except Exception as e:
            error_msg = f"Processing failed for {fund_code}: {str(e)}"
            result["error"] = str(e)

            logger.error(
                "annual_processor.report_failed", fund_code=fund_code, error=str(e)
            )

        return result

    def _save_report_file(
        self, content: bytes, fund_code: str, report_type: str, save_dir: Path
    ) -> Path:
        """ä¿å­˜æŠ¥å‘Šæ–‡ä»¶"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{fund_code}_{report_type}_{timestamp}.xbrl"
        file_path = save_dir / filename

        with open(file_path, "wb") as f:
            f.write(content)

        return file_path

    async def _parse_xbrl_content(self, content: bytes, fund_code: str) -> Dict:
        """è§£æXBRLå†…å®¹"""
        try:
            self.parser.load_from_content(content)

            # æå–ç»“æ„åŒ–æ•°æ®
            fund_info = self.parser.extract_fund_basic_info()
            asset_allocation = self.parser.extract_asset_allocation()
            top_holdings = self.parser.extract_top_holdings()
            industry_allocation = self.parser.extract_industry_allocation()

            return {
                "fund_code": fund_code,
                "parsing_success": True,
                "fund_basic_info": fund_info.__dict__ if fund_info else None,
                "asset_allocation": asset_allocation.__dict__
                if asset_allocation
                else None,
                "top_holdings_count": len(top_holdings) if top_holdings else 0,
                "industry_allocation_count": len(industry_allocation)
                if industry_allocation
                else 0,
                "parsing_time": datetime.now().isoformat(),
            }

        except Exception as e:
            return {
                "fund_code": fund_code,
                "parsing_success": False,
                "parsing_error": str(e),
                "parsing_time": datetime.now().isoformat(),
            }

    def _save_batch_results(self, results: Dict, save_dir: Path):
        """ä¿å­˜æ‰¹é‡å¤„ç†ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = save_dir / f"batch_results_{timestamp}.json"

        with open(results_file, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        logger.info("annual_processor.results_saved", results_file=str(results_file))


async def test_annual_reports_capability():
    """æµ‹è¯•å¹´åº¦æŠ¥å‘Šå¤„ç†èƒ½åŠ›"""
    print("=== æµ‹è¯•2024å¹´å¹´åº¦æŠ¥å‘Šå¤„ç†èƒ½åŠ› ===")

    processor = AnnualReportBatchProcessor()

    try:
        # æµ‹è¯•1: è·å–å°‘é‡å¹´åº¦æŠ¥å‘Šè¿›è¡Œæµ‹è¯•
        print("\n1. è·å–2024å¹´å¹´åº¦æŠ¥å‘Šæ ·æœ¬...")
        sample_reports = await processor.get_all_annual_reports(
            year=2024, max_pages=1  # åªè·å–ç¬¬ä¸€é¡µè¿›è¡Œæµ‹è¯•
        )

        print(f"   è·å–åˆ° {len(sample_reports)} æ¡å¹´åº¦æŠ¥å‘Š")

        if sample_reports:
            print("   ç¤ºä¾‹å¹´åº¦æŠ¥å‘Š:")
            for i, report in enumerate(sample_reports[:5]):
                fund_code = report.get("fund_code", "N/A")
                fund_name = report.get("fund_short_name", "N/A")
                company = report.get("organ_name", "N/A")
                upload_id = report.get("upload_info_id", "N/A")

                print(f"     {i+1}. {fund_code} - {fund_name}")
                print(f"        ç®¡ç†å…¬å¸: {company}")
                print(f"        ä¸Šä¼ ID: {upload_id}")

        # æµ‹è¯•2: æ‰¹é‡ä¸‹è½½å’Œè§£æï¼ˆå°‘é‡æ ·æœ¬ï¼‰
        if sample_reports:
            print(f"\n2. æµ‹è¯•æ‰¹é‡ä¸‹è½½å’Œè§£æ...")
            test_reports = sample_reports[:3]  # åªæµ‹è¯•å‰3ä¸ª

            results = await processor.batch_download_and_parse(
                reports=test_reports,
                save_dir=Path("data/annual_test"),
                max_concurrent=2,
                parse_xbrl=True,
            )

            print(f"   æ‰¹é‡å¤„ç†ç»“æœ:")
            print(f"   æ€»æŠ¥å‘Šæ•°: {results['total_reports']}")
            print(f"   ä¸‹è½½æˆåŠŸ: {results['successful_downloads']}")
            print(f"   ä¸‹è½½å¤±è´¥: {results['failed_downloads']}")
            print(f"   è§£ææˆåŠŸ: {results['successful_parsing']}")
            print(f"   è§£æå¤±è´¥: {results['failed_parsing']}")
            print(f"   æ€»æ–‡ä»¶å¤§å°: {results['statistics']['total_file_size']:,} å­—èŠ‚")

            if results["download_details"]:
                print(f"\n   è¯¦ç»†ç»“æœ:")
                for detail in results["download_details"]:
                    fund_code = detail["fund_code"]
                    fund_name = detail["fund_name"]
                    success = "âœ…" if detail["download_success"] else "âŒ"
                    parsing = "âœ…" if detail.get("parsing_success") else "âŒ"

                    print(f"     {success} {fund_code} - {fund_name}")
                    if detail["download_success"]:
                        print(f"        æ–‡ä»¶å¤§å°: {detail['file_size']:,} å­—èŠ‚")
                        print(f"        XBRLè§£æ: {parsing}")

        return len(sample_reports) > 0

    except Exception as e:
        print(f"   âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
        return False


async def full_batch_download():
    """å®Œæ•´æ‰¹é‡ä¸‹è½½æ‰€æœ‰2024å¹´å¹´åº¦æŠ¥å‘Š"""
    print("=== å®Œæ•´æ‰¹é‡ä¸‹è½½2024å¹´å¹´åº¦æŠ¥å‘Š ===")

    processor = AnnualReportBatchProcessor()

    try:
        # è·å–æ‰€æœ‰å¹´åº¦æŠ¥å‘Š
        print("\n1. è·å–æ‰€æœ‰2024å¹´å¹´åº¦æŠ¥å‘Š...")
        all_reports = await processor.get_all_annual_reports(year=2024)

        print(f"   æ€»å…±æ‰¾åˆ° {len(all_reports)} æ¡2024å¹´å¹´åº¦æŠ¥å‘Š")

        if not all_reports:
            print("   âŒ æœªæ‰¾åˆ°ä»»ä½•å¹´åº¦æŠ¥å‘Š")
            return False

        # ç»Ÿè®¡åŸºé‡‘ç±»å‹
        fund_companies = {}
        qdii_count = 0

        for report in all_reports:
            company = report.get("organ_name", "æœªçŸ¥")
            fund_companies[company] = fund_companies.get(company, 0) + 1

            fund_name = report.get("fund_short_name", "")
            if processor.qdii_service.is_qdii_fund(fund_name):
                qdii_count += 1

        print(f"\n   ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   åŸºé‡‘å…¬å¸æ•°é‡: {len(fund_companies)}")
        print(f"   QDIIåŸºé‡‘æ•°é‡: {qdii_count}")
        print(f"   å‰5å¤§åŸºé‡‘å…¬å¸:")

        for company, count in sorted(
            fund_companies.items(), key=lambda x: x[1], reverse=True
        )[:5]:
            print(f"     {company}: {count} åªåŸºé‡‘")

        # è¯¢é—®ç”¨æˆ·æ˜¯å¦ç»§ç»­
        print(f"\nâš ï¸  å³å°†ä¸‹è½½ {len(all_reports)} ä¸ªåŸºé‡‘çš„å¹´åº¦æŠ¥å‘Š")
        print(f"   é¢„ä¼°æ–‡ä»¶å¤§å°: {len(all_reports) * 100} KB - {len(all_reports) * 500} KB")
        print(
            f"   é¢„ä¼°ä¸‹è½½æ—¶é—´: {len(all_reports) * 2 // 60} - {len(all_reports) * 5 // 60} åˆ†é’Ÿ"
        )

        user_input = input("\næ˜¯å¦ç»§ç»­æ‰¹é‡ä¸‹è½½ï¼Ÿ(y/N): ").strip().lower()

        if user_input != "y":
            print("   ç”¨æˆ·å–æ¶ˆä¸‹è½½")
            return False

        # å¼€å§‹æ‰¹é‡ä¸‹è½½
        print(f"\n2. å¼€å§‹æ‰¹é‡ä¸‹è½½å’Œè§£æ...")
        results = await processor.batch_download_and_parse(
            reports=all_reports,
            save_dir=Path("data/annual_reports_2024_full"),
            max_concurrent=5,  # é€‚åº¦å¹¶å‘
            parse_xbrl=True,
        )

        # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
        print(f"\nğŸ¯ æ‰¹é‡ä¸‹è½½å®Œæˆ!")
        print(f"   æ€»æŠ¥å‘Šæ•°: {results['total_reports']}")
        print(f"   ä¸‹è½½æˆåŠŸ: {results['successful_downloads']}")
        print(f"   ä¸‹è½½å¤±è´¥: {results['failed_downloads']}")
        print(f"   è§£ææˆåŠŸ: {results['successful_parsing']}")
        print(f"   è§£æå¤±è´¥: {results['failed_parsing']}")
        print(
            f"   æˆåŠŸç‡: {(results['successful_downloads'] / results['total_reports'] * 100):.1f}%"
        )
        print(
            f"   æ€»æ–‡ä»¶å¤§å°: {results['statistics']['total_file_size'] / 1024 / 1024:.1f} MB"
        )
        print(f"   QDIIåŸºé‡‘æ•°é‡: {results['statistics']['qdii_funds']}")

        return True

    except Exception as e:
        print(f"   âŒ æ‰¹é‡ä¸‹è½½å¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
        return False


async def main():
    """ä¸»å‡½æ•°"""
    print("2025å¹´å¹´åº¦æŠ¥å‘Šæ‰¹é‡ä¸‹è½½å’Œè§£æå·¥å…·")
    print("=" * 60)

    print("\né€‰æ‹©æ“ä½œæ¨¡å¼:")
    print("1. æµ‹è¯•æ¨¡å¼ - ä¸‹è½½å°‘é‡æŠ¥å‘Šè¿›è¡Œæµ‹è¯•")
    print("2. å®Œæ•´æ¨¡å¼ - ä¸‹è½½æ‰€æœ‰2024å¹´å¹´åº¦æŠ¥å‘Š")
    print("3. é€€å‡º")

    while True:
        choice = input("\nè¯·é€‰æ‹© (1/2/3): ").strip()

        if choice == "1":
            print("\nğŸ§ª å¯åŠ¨æµ‹è¯•æ¨¡å¼...")
            success = await test_annual_reports_capability()
            if success:
                print("\nâœ… æµ‹è¯•æ¨¡å¼å®Œæˆï¼ŒåŠŸèƒ½éªŒè¯æˆåŠŸï¼")
                print("   å¯ä»¥é€‰æ‹©å®Œæ•´æ¨¡å¼ä¸‹è½½æ‰€æœ‰æŠ¥å‘Š")
            else:
                print("\nâŒ æµ‹è¯•æ¨¡å¼å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œé…ç½®")
            break

        elif choice == "2":
            print("\nğŸš€ å¯åŠ¨å®Œæ•´æ¨¡å¼...")
            success = await full_batch_download()
            if success:
                print("\nâœ… å®Œæ•´æ‰¹é‡ä¸‹è½½å®Œæˆï¼")
            else:
                print("\nâŒ æ‰¹é‡ä¸‹è½½å¤±è´¥")
            break

        elif choice == "3":
            print("\nğŸ‘‹ é€€å‡ºç¨‹åº")
            break

        else:
            print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ 1ã€2 æˆ– 3")


if __name__ == "__main__":
    asyncio.run(main())
